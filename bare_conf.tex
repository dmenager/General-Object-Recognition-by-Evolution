\documentclass[conference]{IEEEtran}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{grffile}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{color}
\usepackage{mathtools}
\usepackage{algorithm, algpseudocode}

\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}

\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
\else
\fi

\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\author{\IEEEauthorblockN{David Menager}
\IEEEauthorblockA{School of Electrical Engineering and Computer Science\\
University of Kansas\\
Lawrence, Kansas 66045\\
Email: dhmenager@ku.edu}}

\lstset{language=Matlab,%
    %basicstyle=\color{red},
    breaklines=true,%
    morekeywords={matlab2tikz},
    keywordstyle=\color{blue},%
    morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
    identifierstyle=\color{black},%
    stringstyle=\color{mylilas},
    commentstyle=\color{mygreen},%
    showstringspaces=false,%without this there will be a symbol in the places where there is a space
    numbers=left,%
    numberstyle={\tiny \color{black}},% size of the numbers
    numbersep=9pt, % this defines how far the numbers are from the text
    emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
    %emph=[2]{word1,word2}, emphstyle=[2]{style},    
  }
\title{On the Construction of Evolutionary Features for General Object Recognition}
\maketitle

\begin{abstract}
  Object recognition is an important part in computer vision that that allows many computer programs to see the light of day. There are many open problems in this area, and in this paper, we examine a particular approach that uses evolutionary methods for constructing salient features of images. These features are then combined in an ensemble classifier to predict an object class. The performance of this system is reportedly state-of-the-art, and we present our methodology and show that our system fails to meet this expectation due to the time constraints required for the completion of this paper.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
Many theories in artificial intelligence (AI) become reduced to the realm of ``cool ideas'' without computers that can actualize them in the world in which we live. This is not only true for classical AI tasks such as heuristic search, but for other high level tasks such as completing a search and rescue mission, or inferring other people's goals in an environment. High level tasks such as these fundamentally rely on computer vision modules that allow agents to sense the world. In practice, however, researchers tend to avoid this topic by implementing their solutions in simulations. This allows them to develop strong theories that generalize to diverse environments, but often times the research makes assumptions that the vision problem is already solved. Obviously, this is not true.

There is a wealth of research that aims to tackle the different aspects of computer vision. These works address a breadth of topics such ass 3D reconstruction, remote sensing, and augmented reality. Since we are interested in vision modules for physical agents, we will focus this paper on object recognition. Object recognition is the task of assigning an image to a particular object class. In the standard case, each picture images one object, and thus belongs to one object class. In the supervised context, these classes are known apriori and are associated to their corresponding images in a training set. Then the validation of the models are done by using a hold-out set. A procedure like this bares a strong resemblance to the standard machine learning approach, and in fact object recognition, and pattern matching in general, can be seen as a machine learning problem. This relationship between machine learning and object recognition also implies that in order to produce a quality model, a significant amount of work must be done to clean the data that these models consume. Typically, methods for recognizing objects in images are senstive to:
\begin{description}
\item [Lighting conditions]
  Images of the same object may be subject to different lighting conditions. This difference is potentially enough to mis-classify an object.
\item [Position of the object]
  Many algorithms have better performance if the object to recognize is centered in the image at the foreground. Object recognition becomes significantly more challenging when no constraints are placed on the location of the object.
\item [Orientation of the object]
  Object recognition algorithms can be sensitive to the orientation of the imaged object because these systems analyze the intensity values of pixels, and do not inherrently extract semantic information from images without further processing.
\item [Size of the object]
  As the size of the image object varies, the level of detail will also fluctuate, and so this is another issue object recognition systems must address because the number and quality of features in an object can change.
\item [Amount of occlusion of the object]
  Occluded objects are a significant challenge to object recognition systems. Many important features of an object may be missing because they are hidden by another object. 
\item [etc.]
  Various other factors such as the color of an object, or the number of objects to recognize in an image affect the quality of object recognition systems.
\end{description}

All of these give an intuition of the challenging nature of object recognition tasks. In general, it is very difficult to determine which features of an image help to identify the class of an object, given that it may be subject to various levels of noise. Furthermore, the quality of results obtain by these methods are highly dependent on the quality of the datasets being used to train the vision modules. Object recognition is an important research area in computer vision because the success of other, more high-level tasks like extracting relational, or propositional information from 2D images depend on it.

There are many techniques for object recognition \cite{sukanya2016survey}, however in this paper we aim to do this using an ensemble of perceptrons, who have been trained by evolution. This method, proposed by \cite{lillywhite2013feature} performs comparably to deep architectures built for this task, which yield state-of-the art performance. We begin this paper with a review of the genetic algorithm, then we move to explain our experimental setup and compare it with \cite{lillywhite2013feature}. Following this, we provide our results and close with concluding remarks. 
\section{Genetic Algorithm Review}
Genetic algorithms simulate evolution. The idea is to create a program that solves problems by using the well-established principles of Darwinian evolution. In essence, the aim is to create programs that solve problems without needing to be told {\it how} to solve them \cite{koza1992genetic}. Genetic algorithms first begin by initializing a random population. Every individual in a population is represented as a gene vector, where each element in that vector is a feature of that individual. Each individual attempts the problem and is assigned a fitness score based on how well the individual solves that problem. Based on the fitness score, the most fit individuals are selected to pass on their genes to the next generation. Through the processes of sexual recombination (cross over) and mutation, the next generation is created. This process continues for $N$ generations, or until some other stopping criteria is reached.

The cross over procedure is quite simple. In order to generate a new individual from sexual recombination, it suffices to choose a cross over point in each parent. Then, choose the parent whos genes form the beginning of the child, and stitch the end of the second parent's genes to the end of the new child's. After a child has been created in this way, mutations may occur that change the child's specific feature values. More intense mutations may occur by deleting or adding entire elements. From this, it is easily seen that genetic algorithms can span the set of possible solutions because solutions are of non-uniform size and mutations allow the solutions to explore new places in the search space.

Genetic algorithms are attractive particularly because of their broad applicability to optimization problems. As long as a problem can be stated in terms of the aforementioned process, genetic algorithms are good candidates for finding high quality solutions to them. Other than the formalisms inherrently embedded in the procedure, and knowledge about the problem do main, no other mathematic constructions are required. For instance, the problem optimal control may be approached with a genetic algorithm. If the programmer is trying to control a UAV and knows what low-level motor actions the UAV can perform, then minimizing the error between the craft's actual state and the desired state becomes a task for evolution. 

In a formal setting, genetic algorithms solve problems by an approach similar to gradient ascent, calld {\it hill climbing}. In gradient ascent, the optimal solution is found by going in the direction of the gradient. On a 3-dimensional convex function, going up the gradient will potentially change all three values of the solution vector instantaneously. This contrasts with hill climbing because, for the case of hill climbing, generating the next solution only involves modifying a set of components from the original solution vector. In other words, new solutions are created by copying part of an old solution and adding a set of new pieces that complete the solution. It may not be immediately clear to the reader, but applying cross over with two parent individuals is hill climbing in the context of the genetic algorithm.

\subsection{Why it Works}
Now, we consider why the genetic algorithm works. We do not supply a formal proof, but give a strong intuition. The reason why genetic algorithms work is because in each generation they select the most fit individuals to produce the next. Therefore the average fitness of each sucessive generation is non-decreasing. DO MORE WORK HERE.
\section{Experimental Comparison}
In this section we take the time to compare our experimental setup with that of \cite{lillywhite2013feature}. We begin by discussing the commonalities and procede to discuss the differences.
\subsection{Comonalities in approach}
Here we describe, in large part the original paper's implementation and formalisms. In our project, our aim was to remain as faithful to the original theory as possible. We largly embrace the same formalisms, but deviate mostly in the system parameters
\subsubsection{ECO Features and Perceptrons}
Perhaps, the strongest commonality to mention is that our work is a reimplementation of Lillywhite et al. The task is to devise an evolutionaray algorithm to construct a set of features that improve a classification algorithm's ability to distinguish object classes. For this reason, the features are called {\it ECO features}, because they are {\bf E}volutionarily {\bf Co}nstructed. An ECO feature, $V$ is simply an ordering of image transformations with their accompanying parameters and is fomalized as
\begin{equation}
  \begin{split}
    V_n & = T_n(V_{n-1}, \phi)\\
    V_{n-1} & = T_{n-1}(V_{n-2},\phi_{n-1})\\
    \vdots\\
    V_1 & = T_1(I(x_1,y_1,x_2,y_2), \phi_{1}),
  \end{split}
  \label{eq:eco}
\end{equation}
where $I$ is the original image, $T_i$ is an image transform, and $\phi$ is the set of corresponding parameters to that transform. Lillywhite et al. also describe the ability to apply these ECO features to particular subregions of the image. The subregion is defined by $x_1, x_2, y_1,$ and $y_2$, but we chose not to implement the subregions feature because the original authors obtained comprable results with the subregions turned off as well.

Furthermore, Lillywhite et al provide a table to show the set of transformations their algorithm employs. In our implementation we only use a subset of these transforms because we utilized another library. Our specific transformations are shown in Table~\ref{tab:transforms}. 
\begin{table}[t]
  \centering
  \begin{tabular}{c c c c}
    \hline
    Image Transform & $\abs{\phi}$ & Image Transform & $\abs{\phi}$\\
    \hline
    Gabor & 9 & Rank Order & 1\\
    Gradient & 2 & Sobel & 1\\
    Gaussian & 2 & Erosion & 2\\
    Histogram & 2 & Hough Line & 1\\
    Hough Circle & 3 & Equalize Histogram & 2\\
    Median Blur & 2 & Laplace Edge & 2\\
    Integral Blur & 1 & Dilation & 2\\
    Canny Edge & 4\\
    \hline
  \end{tabular}
  \caption{Image transforms used in project}
  \label{tab:transforms}
\end{table}

Since this is a genetic algorithm, first generation is initialized randomly. Each candidate ECO feature has an associated perceptron that is used to evaluate the quality of the transformed images. Each pixel in the image is multiplied by its corrsponding perceptron weights, the result of which is summed. If the sum is greater than a threshold, ``1'' is the output, otherwise the perceptron outputs 0. The equation for the perceptron output, $\alpha$ is described as the following:
\begin{equation}
  \alpha =
  \begin{cases}
    1, & \text{if } W \cdot V + b > 0\\
    0, & \text{otherswise}
    \end{cases},
  \label{eq:perceptron-output}
\end{equation}
where $W$ is the perceptron weight vector. Furthermore, each image has a class associated that can be discretized as, $\beta$ which is ``1'' for the first image class and ``0'' for the second. So, the error, $\delta$ of the perceptron is written as
\begin{equation}
  \delta = \beta - \alpha.
  \label{eq:perceptron-error}
\end{equation}
This means that training the perceptron is characterized by
\begin{equation}
  W[i] = W[i] + \lambda \cdot \delta \cdot V[i],
  \label{eq:perceptron-update}
\end{equation}
where $\lambda$ is the learning rate. This parameter controls the volatility of the network. High learning rates will cause a network to rapidly oscilate between extreme values. This comes as a trade off to small learning rates that move to convergence. Both high and low learning rates have their disadvantages, however. High learning rates may never converge because the weights bounce around the space of network weights. On the other hand, small learning rates may converge to local maxima and never find an optimal solution. So, the best learning rate, must be found experimentally.

Because Equation~\ref{eq:perceptron-output} through Equation~\ref{eq:perceptron-update} define the fundamental procedures that operate on perceptrons, a fitness function can be defined. Lillywhite et al. specify theirs as

\begin{equation}
  s = \frac{t_p \cdot 500}{f_n + t_p} + \frac{t_n \cdot 500}{p \cdot f_p+ t_n},
  \label{eq:fitness-lilly}
\end{equation}
because it is unaffected by unbalanced classes, and it includes an extra penalty for false positives. In this equation, $t_p, t_n, f_p,$ and $f_n$ stand for true positives, true negative,s false posiives, and false negatives respectfully.

\subsubsection{Genetic Algorithm Structure}
Although we discussed the basic genetic algorithm in the previous section, we have yet to define its specific implementation. Lillywhite et al.'s algorithm for evolving features is as follows
\begin{algorithm}[t]
  \caption{Genetic Algorithm Outline}
  \label{alg:genetic}
  \begin{algorithmic}
    \For{Size of population}
    \State Randomly create creatures
    \EndFor
    \For{Number of generations}
    \For{Every Creature}
    \For{Every Training Image}
    \State Process the image with feature transformations
    \State Train creature's perceptron
    \EndFor
    \For{Every holding image}
    \State Process the image with feature transformations
    \EndFor
    \State Calculate fitness score
    \State Save creature if sufficiently fit
    \EndFor
    \State Create the next generation
    \EndFor
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
  \caption{Train Adaboost}
  \label{alg:ada}
  \begin{algorithmic}
    \State Set of training images M
    \For{Every training image, m}
    \State Initialize $\delta_m[m]=1/|M|$
    \EndFor
    \For{$x=0 \text{ to } X$}
    \For{Every perceptron, $w$}
    \For{Every training image, $m$}
    \If{Wrongly classified}
    \State $\delta_w+=\delta_M[m]$
    \EndIf
    \EndFor
    \EndFor
    \State Select perceptron with minimum Error
    \If{$\delta_\omega[\Omega]>=50\%$}
    \State BREAK
    \EndIf
    \State Calculate perceptron coefficient using
    \State$\rho=\frac{1}{2} \cdot ln\frac{1-\delta_w}{\delta_w}$
    \For{Every training image, $m$}
    \State
    $c =
    \begin{cases}
      1, & \text{if classified correctly}\\
      -1, & \text{otherswise}
    \end{cases}$
    \State $\delta_M[m]=\frac{\delta_M[m]*e^{-\rho \cdot c}}{\text{Normalization factor}}$
    \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}

Note, that in Algorithm~\ref{alg:genetic} Lillywhite et al. do not specify how they split their data into training and testing, and more importantly, they do not explain how they process images through the perceptron. Each image has a different size, so in our implementation we fix the number of perceptrons to $\text{max image size } \times \text{max image size}$. Also, observe that once sufficiently fit features are found, the are saved in the list of true ECO features. This list is passed on to create the Adaboost classifier. This procedure is described by Agorithm~\ref{alg:ada}.

Our project is a reimplementation of \cite{lillywhite2013feature} that aimed to remain faithful to as many of the theoretical commitments made by the original authors. For the most part we succeeded in achieving this goal, however, external factors such as the time constraint placed on the completion of this project forced us to scale down a few minor aspects of the code. We describe these in the following subsection.

\subsection{Differences}
We begin with a discussion on the difference in parameter settings. Lillywhite et al.'s results are generated by an initial population of 1000 individuals who evolve over 100 generations. An algorithmically complex code such as this with the same parameter settings would take the majority of a month to complete on our machines. So, the first modification was to create a population of 10 individuals who evolve over 10 generations. The reader might correctly suspect that this is much too short a time to reach convergence. We do not expect our experiment to converge. Rather, we aim to demonstrate the process required to run this genetic algorithm, and we aim to provide preliminary evidence that the evolutionary process is working.

Secondly, Lillywhite et al. state that they only required the initial generation to be no more than eight features long. They also report that the genetic algorithm naturally preferred features with fewer transforms. We are skeptical of this partially because the fitness function does not penalize an individual on the size of it features. We chose to be more restrictive, and specify that all individuals may have at maximum five feature transformations. This specification was a way to artificially append a constraint to the fitness function.

Thirdly, the original paper lacks details with respect to creating generations. The paper does not describe the order in which parents are recombined, does not discuss the number of children two individuals can generate, and the paper is silent about what types of mutations are allowed. In our case, we decided to allow mutations to the transformations and the transformation parameters. We do not add or delete transformations.

Finally, because of the small number of individuals and generations we have a learning rate of 1, mutation probability of .35, add no bias term in Equation~\ref{eq:perceptron-output}, and we include no penalty in our fitness function. The reason why we disagree with the fitness function described in Equation~\ref{eq:fitness-lilly} is because it has poor worst-case performance. We identify two worst-case scenarios:
\begin{enumerate}
\item The perceptron output seems random
\item The perceptron output is constant
\end{enumerate}

In the first case, the fitness function performs reasonably well and penalizes the perceptron for its deviations. The second case contains two subcases, one for each value. If the perceptron output is 1, then Equation~\ref{eq:fitness-lilly} becomes

\begin{equation}
  \begin{split}
    s & = \frac{t_p \cdot 500}{f_n + t_p} + \frac{t_n \cdot 500}{p \cdot f_p+ t_n}\\
    & = \frac{t_p \cdot 500}{0 + t_p} + \frac{0 \cdot 500}{p \cdot f_p + 0}\\
    & = 500(1)
  \end{split}.
  \label{eq:fitness-lilly-decompose}
\end{equation}
For the second subcase, we obtain

\begin{equation}
  \begin{split}
    s & = \frac{t_p \cdot 500}{f_n + t_p} + \frac{t_n \cdot 500}{p \cdot f_p+ t_n}\\
    & = \frac{0 \cdot 500}{f_n + 0} + \frac{t_n \cdot 500}{p \cdot 0 + t_n}\\
    & = 500(1)
  \end{split},
  \label{eq:fitness-lilly-decompose2}
\end{equation}
So, in both cases, if the perceptron does not try to distinguish any classes, it recieves a fitness score of 500. On the other hand, a perceptron that attempts to distinguish the two classes will be penalized more because the denominator will include terms for Type I and Type II errors. Furthermore, false positives are penalized heavily. Since the initial generation is created randomly, the features that pass on their traits will be the ones that hav no ability to distinguish classes, and the average fitness of the generations will remain at 500. We have empirically seen that ECO features with this trait contain more than 6 transforms.

Our new fitness function shown in Equation~\ref{eq:fitness-menager} avoids this issue in a three-fold manner. First, we include $L_2$ regularization on the length of the feature. Second, our fitness function accounts for the accuracy, precision, and recall of the perceptron, and third, provides a weighting mechanism to prefer accuracy over precision and recall, (and vise versa). This formulation
\begin{equation}
  \begin{split}
    s & = \alpha(\frac{t_p + t_n}{t_p + t_n + f_p + f_n}) +\\
    &(1 - \alpha)(\frac{t_p}{t_p + f_p} + \frac{t_p}{t_p + f_n}) + \lambda \sqrt{\sum_{\bf T} |T_i|}
    \end{split}
  \label{eq:fitness-menager}
\end{equation}
penalizes errors in classification, but also heavily penalizes indistinguishabliity by applying some weightage on the accuracy, precision and recall. In our work, we set $\alpha$ to .4, and the regularization parameter, $\lambda$ is set to 1.

After using this fitness function, we realized that the length of the ECO features were still quite long, so this motivated us to impose the strict 5-feature limit, as mentioned earlier. As future work, we would like to experiment with other regularizers, like using the $L_1$ norm. The downside to this is that this fitness function is sensitive to class imbalance, unlike the one used by Lillywhite et al. 

\section{Results}
\section{Conclusion}
\section{Appendix}
%\lstinputlisting{GA.lisp}
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,menager}
\end{document}


